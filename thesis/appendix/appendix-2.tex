\chapter*{پیوست ۲ - آموزش مدل با روش \lr{QLORA}}

\begin{latin}
    \begin{lstlisting}[language=Python, title=\rl{بارگذاری داده ها و ایجاد لیبل ها بصورت \lr{one hot}}]
df = pd.read_csv('prepared_dataset/dataset.csv', header=None,
 names=['file_path', 'codesmells'])
df.rename(columns={'codesmells': 'labels'}, inplace=True)
df['labels'] = df['labels'].apply(lambda x: x.split(','))

all_labels = set(label for sublist in df['labels'] for label in sublist)
label_to_idx = {label: idx for idx, label in enumerate(all_labels)}
id2label = {i: label for label, i in label_to_idx.items()}

def encode_labels(labels):
    encoded = [0] * len(label_to_idx)
    for label in labels:
        encoded[label_to_idx[label]] = 1
    return encoded
	df['encoded_labels'] = df['labels'].apply(encode_labels)
\end{lstlisting}
\end{latin}

\begin{latin}
    \begin{lstlisting}[language=Python, title=\rl{جدا کردن داده های آزمون و آموزش و ساخت دیتاست}]
train_df, test_df = train_test_split(df, test_size=0.1, random_state=42, shuffle=True)
class CodeDataset(Dataset):
    def __init__(self, dataframe,tokenizer):
        self.dataframe = dataframe
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        code_path = os.path.join("prepared_dataset","output_code",
		self.dataframe.iloc[idx]['file_path'])
        with open(code_path, 'r') as file:
            code = file.read()
        labels = torch.tensor(self.dataframe.iloc[idx]['encoded_labels'],
		 dtype=torch.float).to(device)
        inputs = self.tokenizer(code, return_tensors='pt',
		 truncation=True,padding='max_length', max_length = MAX_LEN,
		 add_special_tokens = True).to(device)

        #squeeze inputs:
        inputs = {key: val.squeeze() for key, val in inputs.items()}
        return {**inputs, 'labels': labels}
train_dataset = CodeDataset(train_df,tokenizer)
test_dataset = CodeDataset(test_df,tokenizer)
\end{lstlisting}
\end{latin}

\begin{latin}
    \begin{lstlisting}[language=Python, title=\rl{بارگذاری توکنایزر و مدل و کوانتیزه کردن مدل}]
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)
model = AutoModelForSequenceClassification.from_pretrained(
MODEL_NAME,num_labels=len(all_labels),
quantization_config=quantization_config,
problem_type="multi_label_classification",
low_cpu_mem_usage=True)
\end{lstlisting}
\end{latin}

\begin{latin}
    \begin{lstlisting}[language=Python, title=\rl{استفاده از حالت مصرف بهینه رم برای پارامتر ها با استفاده از \lr{PEFT}}]
model.train() # model in training mode (dropout modules are activated)

# enable gradient check pointing
model.gradient_checkpointing_enable()

# enable quantized training
model = prepare_model_for_kbit_training(model)
# LoRA config
config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type="SEQ_CLS",
)
# LoRA trainable version of model
model = get_peft_model(model, config)

# trainable parameter count
model.print_trainable_parameters()
\end{lstlisting}
\end{latin}

\begin{latin}
    \begin{lstlisting}[language=Python, title=\rl{آموزش دادن مدل}]
training_args = TrainingArguments(
	output_dir='./results',
	evaluation_strategy="epoch",
	learning_rate=2e-5,
	warmup_steps=500,
	per_device_train_batch_size=BATCH_SIZE,
	per_device_eval_batch_size=BATCH_SIZE,
	num_train_epochs=NUM_EPOCHS,
	weight_decay=0.01,
	gradient_accumulation_steps=4,
	# warmup_steps=2,
	fp16=True,
	optim="paged_adamw_8bit",
)
trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics,
)
model.config.use_cache = False  # silence the warnings. Please re-enable for inference!
trainer.train()
model.config.use_cache = True
\end{lstlisting}
\end{latin}
\clearpage